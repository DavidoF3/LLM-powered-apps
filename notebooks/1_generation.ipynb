{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "To build a chatbot app, we need a set of questions and answers. This is helpful for evaluating different prompt engineering techniques and different app design choices.\n",
    "\n",
    "In this notebook we dive deeper on prompting the model by passing a better context by:\n",
    "* using available data of W&B user questions \n",
    "* using the documentation files to generate better answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from getpass import getpass\n",
    "\n",
    "from rich.markdown import Markdown\n",
    "import pandas as pd\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential, # for exponential backoff\n",
    ")  \n",
    "import wandb\n",
    "from wandb.integration.openai import autolog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set OpenAI API key \n",
    "\n",
    "To get key, click on [link](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key configured\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "  openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "print(\"OpenAI API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start W&B logging\n",
    "\n",
    "autolog - convenient function for logging OpenAI results to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-wind-2</strong> at: <a href='https://wandb.ai/doc93/llmapps/runs/p24nnwgw' target=\"_blank\">https://wandb.ai/doc93/llmapps/runs/p24nnwgw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230819_001319-p24nnwgw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/davidoc/Documents/personal/LLM-powered-apps/notebooks/wandb/run-20230819_010617-ti93ixrt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/doc93/llmapps/runs/ti93ixrt' target=\"_blank\">unique-frost-3</a></strong> to <a href='https://wandb.ai/doc93/llmapps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/doc93/llmapps' target=\"_blank\">https://wandb.ai/doc93/llmapps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/doc93/llmapps/runs/ti93ixrt' target=\"_blank\">https://wandb.ai/doc93/llmapps/runs/ti93ixrt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "autolog({\"project\":\"llmapps\", \"job_type\": \"generation\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating synthetic support questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completion_with_backoff \n",
    "# - this decorator will make API request wait if it hits a rate limiting error\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.ChatCompletion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "# MODEL_NAME = \"gpt-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot prompting\n",
    "\n",
    "Not giving any examples or context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the behaviour, qualities of the LLM\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "# Define what we ask the LLM to do\n",
    "user_prompt = \"Generate a support question from a W&B user\"\n",
    "\n",
    "def generate_and_print(system_prompt, user_prompt, n=5):\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    responses = completion_with_backoff(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        n = n,\n",
    "        )\n",
    "    for response in responses.choices:\n",
    "        generation = response.message.content\n",
    "        display(Markdown(generation))\n",
    "    \n",
    "generate_and_print(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 generated responses above are quite generic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot prompting \n",
    "\n",
    "Give the model several examples.\n",
    "* We read some user submitted questions (from Discord server) listed in the file `examples.txt`. \n",
    "* This file contains multiline questions separated by tabs (`\\t`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'We have 228 real queries:'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sample one: \"hi  how to pass w&amp;b api key in my kaggle account by using wandblogger from pytorch lightinh module ?\" \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Sample one: \"hi  how to pass w&b api key in my kaggle account by using wandblogger from pytorch lightinh module ?\" \n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delimiter = \"\\t\" # tab separated queries\n",
    "with open(\"../data/examples.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "    real_queries = data.split(delimiter)\n",
    "\n",
    "pprint(f\"We have {len(real_queries)} real queries:\")  \n",
    "Markdown(f\"Sample one: \\n\\\"{random.choice(real_queries)}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Few Shot prompt:\n",
    "* we can now add a few of those real user questions to the prompt, to guide our model to produce synthetic questions like those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generate a support question from a W&amp;B user Below you will find a few examples of real user queries: where can I   \n",
       "find my run_id Hi I have created an academic team, however I don't have admin rights in my own group and cannot    \n",
       "access admin features at all Hi, I m training for 100 epochs and logging results and I see around 200 steps on     \n",
       "wandb dashboard. is epoch not same as step ? Let's start!                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generate a support question from a W&B user Below you will find a few examples of real user queries: where can I   \n",
       "find my run_id Hi I have created an academic team, however I don't have admin rights in my own group and cannot    \n",
       "access admin features at all Hi, I m training for 100 epochs and logging results and I see around 200 steps on     \n",
       "wandb dashboard. is epoch not same as step ? Let's start!                                                          \n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_few_shot_prompt(queries, n=3):\n",
    "    prompt = \"Generate a support question from a W&B user\\n\" +\\\n",
    "        \"Below you will find a few examples of real user queries:\\n\"\n",
    "    for _ in range(n):\n",
    "        prompt += random.choice(queries) + \"\\n\"\n",
    "    prompt += \"Let's start!\"\n",
    "    return prompt\n",
    "\n",
    "generation_prompt = generate_few_shot_prompt(real_queries)\n",
    "\n",
    "# Print the prompt to be fitted into the LLM\n",
    "Markdown(generation_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI `Chat` models are very good at following instructions with a few examples. \n",
    "\n",
    "Below we can see how it does by using some context from the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_print(system_prompt, user_prompt=generation_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions produced by LLM, by passing some example questions, are a bit more diverse than with Zero Shot prompting, but we can go further (to increase range of questions produced by LLM).\n",
    "\n",
    "Something that we could add to the prompt to avoid words like \"Sure, ...\" could be: \"Just give me the question, don't give me extra text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Context & Response\n",
    "\n",
    "We want to be able to respond questions that also have some documentation available. As long as we have the documentation for a specific user question, we should be able to answer that question.\n",
    "\n",
    "To evaluate the model, we want to make sure that whenever documentation is available, the answer is correct for that question.\n",
    "* So why not use documentation to also generate synthetic questions?\n",
    "\n",
    "To do this, the folder `../docs_sample` contains several examples of wandb docs. Dataset of questions will be limited to what is available in this docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'edu'...\n",
      "remote: Enumerating objects: 2470, done.\u001b[K\n",
      "remote: Counting objects: 100% (1013/1013), done.\u001b[K\n",
      "remote: Compressing objects: 100% (372/372), done.\u001b[K\n",
      "remote: Total 2470 (delta 712), reused 843 (delta 633), pack-reused 1457\u001b[K\n",
      "Receiving objects: 100% (2470/2470), 22.60 MiB | 7.51 MiB/s, done.\n",
      "Resolving deltas: 100% (1408/1408), done.\n"
     ]
    }
   ],
   "source": [
    "# check if directory exists, if not, create it and download the files, e.g if running in colab\n",
    "if not os.path.exists(\"../docs_sample/\"):\n",
    "  !git clone https://github.com/wandb/edu.git\n",
    "  !cp -r edu/llm-apps-course/docs_sample ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_md_files(directory):\n",
    "    \"Find all markdown files in a directory and return their content and path\"\n",
    "    md_files = []\n",
    "    for file in Path(directory).rglob(\"*.md\"):\n",
    "        with open(file, 'r', encoding='utf-8') as md_file:\n",
    "            content = md_file.read()\n",
    "        md_files.append((file.relative_to(directory), content))\n",
    "    return md_files\n",
    "\n",
    "documents = find_md_files('../docs_sample/')\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the documents are not too long for our context window (prompt), by computing the number of tokens in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4179, 365, 1206, 2596, 2940, 537, 956, 803, 1644, 2529, 2093]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(MODEL_NAME)\n",
    "tokens_per_document = [len(tokenizer.encode(document)) for _, document in documents]\n",
    "pprint(tokens_per_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the documents are too long (don't need that much text in our prompt). For thos documents, we'll extract a random chunk from them - to inspire the LLM to generate ome questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a random chunk from a document\n",
    "def extract_random_chunk(document, max_tokens=512):\n",
    "    tokens = tokenizer.encode(document)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return document\n",
    "    start = random.randint(0, len(tokens) - max_tokens)\n",
    "    end = start + max_tokens\n",
    "    return tokenizer.decode(tokens[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use extracted chunk to create a question that can be answered by the document. This way we can generate questions that our current documentation is capable of answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_prompt(chunk):\n",
    "    prompt = \"Generate a support question from a W&B user\\n\" +\\\n",
    "        \"The question should be answerable by provided fragment of W&B documentation.\\n\" +\\\n",
    "        \"Below you will find a fragment of W&B documentation:\\n\" +\\\n",
    "        chunk + \"\\n\" +\\\n",
    "        \"Let's start!\"\n",
    "    return prompt\n",
    "\n",
    "chunk = extract_random_chunk(documents[0][1])\n",
    "generation_prompt = generate_context_prompt(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generate a support question from a W&amp;B user The question should be answerable by provided fragment of W&amp;B          \n",
       "documentation. Below you will find a fragment of W&amp;B documentation: data = [[wandb.Image(x_i), y_i, y_pred] f or   \n",
       "x_i, y_i, y_pred in list(zip(x[:n], y[:n], outputs[:n]))] wandb_logger.log_table( key='sample_table',              \n",
       "columns=columns, data=data)                                                                                        \n",
       "...                                                                                                                \n",
       "\n",
       "trainer = pl.Trainer( ... callbacks=[LogPredictionSamplesCallback()] )                                             \n",
       "\n",
       "<span style=\"background-color: #272822\">                                                                                                                   \n",
       "                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> ### How to use multiple GPUs with Lightning and W&amp;B?                                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> PyTorch Lightning has Multi-GPU support through their DDP Interface. However, PyTorch Lightning's design requires </span>\n",
       "<span style=\"background-color: #272822\"> us to be careful about how we instantiate our GPUs.                                                               </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> Lightning assumes that each GPU (or Rank) in your training loop must be instantiated in exactly the same way - wi </span>\n",
       "<span style=\"background-color: #272822\"> the same initial conditions. However, only rank 0 process gets access to the `wandb.run` object, and for non-zero </span>\n",
       "<span style=\"background-color: #272822\"> rank processes: `wandb.run = None`. This could cause your non-zero processes to fail. Such a situation can put yo </span>\n",
       "<span style=\"background-color: #272822\"> in a **deadlock** because rank 0 process will wait for the non-zero rank processes to join, which have already    </span>\n",
       "<span style=\"background-color: #272822\"> crashed.                                                                                                          </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> For this reason, we have to be careful about how we set up our training code. The recommended way to set it up    </span>\n",
       "<span style=\"background-color: #272822\"> would be to have your code be independent of the `wandb.run` object.                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> ```python                                                                                                         </span>\n",
       "<span style=\"background-color: #272822\"> class MNISTClassifier(pl.LightningModule):                                                                        </span>\n",
       "<span style=\"background-color: #272822\">     def __init__(self):                                                                                           </span>\n",
       "<span style=\"background-color: #272822\">         super(MNISTClassifier, self).__init__()                                                                   </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         self.model = nn.Sequential(                                                                               </span>\n",
       "<span style=\"background-color: #272822\">             nn.Flatten(),                                                                                         </span>\n",
       "<span style=\"background-color: #272822\">             nn.Linear(28 * 28, 128),                                                                              </span>\n",
       "<span style=\"background-color: #272822\">             nn.ReLU(),                                                                                            </span>\n",
       "<span style=\"background-color: #272822\">             nn.Linear(128, 10),                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         )                                                                                                         </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         self.loss = nn.CrossEntropyLoss()                                                                         </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">     def forward(self, x):                                                                                         </span>\n",
       "<span style=\"background-color: #272822\">         return self.model(x)                                                                                      </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">     def training_step(self, batch, batch_idx):                                                                    </span>\n",
       "<span style=\"background-color: #272822\">         x, y = batch                                                                                              </span>\n",
       "<span style=\"background-color: #272822\">         y_hat = self.forward(x)                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         loss = self.loss(y_hat, y)                                                                                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         self.log(\"train/loss\", loss)                                                                              </span>\n",
       "<span style=\"background-color: #272822\">         return {\"train_loss\": loss}                                                                               </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">     def validation_step(self, batch, batch_idx):                                                                  </span>\n",
       "<span style=\"background-color: #272822\">         x, y = batch                                                                                              </span>\n",
       "<span style=\"background-color: #272822\">         y_hat = self.forward(x)                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         loss = self.loss(y_hat, y)                                                                                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         self.log(\"val/loss\", loss)                                                                                </span>\n",
       "<span style=\"background-color: #272822\">         return {\"val_loss\": loss}                                                                                 </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">     def configure_optimizers(self):                                                                               </span>\n",
       "<span style=\"background-color: #272822\">         return torch.optim.Adam(self.parameters(), lr=0.001)                                                      </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> def main():                                                                                                       </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> Let's start!                                                                                                      </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   \n",
       "</span></pre>\n"
      ],
      "text/plain": [
       "Generate a support question from a W&B user The question should be answerable by provided fragment of W&B          \n",
       "documentation. Below you will find a fragment of W&B documentation: data = [[wandb.Image(x_i), y_i, y_pred] f or   \n",
       "x_i, y_i, y_pred in list(zip(x[:n], y[:n], outputs[:n]))] wandb_logger.log_table( key='sample_table',              \n",
       "columns=columns, data=data)                                                                                        \n",
       "...                                                                                                                \n",
       "\n",
       "trainer = pl.Trainer( ... callbacks=[LogPredictionSamplesCallback()] )                                             \n",
       "\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \n",
       "\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m### How to use multiple GPUs with Lightning and W&B?                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mPyTorch Lightning has Multi-GPU support through their DDP Interface. However, PyTorch Lightning's design requires\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mus to be careful about how we instantiate our GPUs.                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mLightning assumes that each GPU (or Rank) in your training loop must be instantiated in exactly the same way - wi\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mthe same initial conditions. However, only rank 0 process gets access to the `wandb.run` object, and for non-zero\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mrank processes: `wandb.run = None`. This could cause your non-zero processes to fail. Such a situation can put yo\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34min a **deadlock** because rank 0 process will wait for the non-zero rank processes to join, which have already   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mcrashed.                                                                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mFor this reason, we have to be careful about how we set up our training code. The recommended way to set it up   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mwould be to have your code be independent of the `wandb.run` object.                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m```python                                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mclass MNISTClassifier(pl.LightningModule):                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    def __init__(self):                                                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        super(MNISTClassifier, self).__init__()                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        self.model = nn.Sequential(                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m            nn.Flatten(),                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m            nn.Linear(28 * 28, 128),                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m            nn.ReLU(),                                                                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m            nn.Linear(128, 10),                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        )                                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        self.loss = nn.CrossEntropyLoss()                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    def forward(self, x):                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        return self.model(x)                                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    def training_step(self, batch, batch_idx):                                                                   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        x, y = batch                                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        y_hat = self.forward(x)                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        loss = self.loss(y_hat, y)                                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        self.log(\"train/loss\", loss)                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        return {\"train_loss\": loss}                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    def validation_step(self, batch, batch_idx):                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        x, y = batch                                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        y_hat = self.forward(x)                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        loss = self.loss(y_hat, y)                                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        self.log(\"val/loss\", loss)                                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        return {\"val_loss\": loss}                                                                                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    def configure_optimizers(self):                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        return torch.optim.Adam(self.parameters(), lr=0.001)                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mdef main():                                                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mLet's start!                                                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \n",
       "\u001b[0m"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(generation_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate 3 possible questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_print(system_prompt, generation_prompt, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some output questions above seem synthetic (not that related to wandb, but more about some specific coding concept). There are further prompt engineering steps to improve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 5 prompt structure\n",
    "\n",
    "This prompt structure has a complex directive that includes:\n",
    "* Description of high-level goal \n",
    "* And few short examples\n",
    "* Detailed bulleted list of sub-tasks \n",
    "* An explicit statement asking the LLM to explain its output\n",
    "* Guidelines on how LLM output will be evaluated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use GPT4 from here, as it gives better answers and abides to instructions better\n",
    "MODEL_NAME = \"gpt-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System and user templates\n",
    "\n",
    "Here we attempt to create a prompt that follows these Level 5 directions. We split the prompt split into:\n",
    "* **System template** (system message) - instructing model to get into a specific role\n",
    "* **User template** (input from the user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read system_template.txt file into an f-string\n",
    "with open(\"../data/system_template.txt\", \"r\") as file:\n",
    "    system_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You are a creative assistant with the goal to generate a synthetic dataset of Weights &amp; Biases (W&amp;B) user          \n",
       "questions. W&amp;B users are asking these questions to a bot, so they don't know the answer and their questions are    \n",
       "grounded in what they're trying to achieve. We are interested in questions that can be answered by W&amp;B             \n",
       "documentation. But the users don't have access to this documentation, so you need to imagine what they're trying to\n",
       "do and use according language.                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "You are a creative assistant with the goal to generate a synthetic dataset of Weights & Biases (W&B) user          \n",
       "questions. W&B users are asking these questions to a bot, so they don't know the answer and their questions are    \n",
       "grounded in what they're trying to achieve. We are interested in questions that can be answered by W&B             \n",
       "documentation. But the users don't have access to this documentation, so you need to imagine what they're trying to\n",
       "do and use according language.                                                                                     \n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read prompt_template.txt file into an f-string\n",
    "with open(\"../data/prompt_template.txt\", \"r\") as file:\n",
    "    prompt_template = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "{QUESTIONS}                                                                                                        \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "In the next step, you will read a fragment of W&amp;B documentation. This will serve as inspiration for synthetic user \n",
       "question and the source of the answer. Here is the document fragment:                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "{CHUNK}                                                                                                            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "You will now generate a user question and corresponding answer based on the above document. First, explain the user\n",
       "context and what problems they might be trying to solve. Second, generate user question. Third, provide the        \n",
       "accurate and concise answer in markdown format to the user question using the documentation. You'll be evaluated   \n",
       "on:                                                                                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>how realistic is that this question will come from a real user one day?                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>is this question about W&amp;B?                                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>can the question be answered using the W&amp;B document fragment above?                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>how accurate is the answer? Remember that users have different styles and can be imprecise. You are very good at\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>impersonating them! Use the following format: CONTEXT: QUESTION: ANSWER: Let's start!                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "{QUESTIONS}                                                                                                        \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "In the next step, you will read a fragment of W&B documentation. This will serve as inspiration for synthetic user \n",
       "question and the source of the answer. Here is the document fragment:                                              \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "{CHUNK}                                                                                                            \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "You will now generate a user question and corresponding answer based on the above document. First, explain the user\n",
       "context and what problems they might be trying to solve. Second, generate user question. Third, provide the        \n",
       "accurate and concise answer in markdown format to the user question using the documentation. You'll be evaluated   \n",
       "on:                                                                                                                \n",
       "\n",
       "\u001b[1;33m • \u001b[0mhow realistic is that this question will come from a real user one day?                                         \n",
       "\u001b[1;33m • \u001b[0mis this question about W&B?                                                                                     \n",
       "\u001b[1;33m • \u001b[0mcan the question be answered using the W&B document fragment above?                                             \n",
       "\u001b[1;33m • \u001b[0mhow accurate is the answer? Remember that users have different styles and can be imprecise. You are very good at\n",
       "\u001b[1;33m   \u001b[0mimpersonating them! Use the following format: CONTEXT: QUESTION: ANSWER: Let's start!                           \n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above prompt, we tell the model:\n",
    "* We say that we provide examples of real user question (this is the **few shot** part of the prompt)\n",
    "* {Need to provide examples}  \n",
    "* We provide fragment of W&B docs for inspiration for synthetic questions and source of answer\n",
    "* {Need to provide docs}\n",
    "* Provide further info to the model to guide the model answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, below, we fill above template prompt by using\n",
    "* Example questions from **[examples.txt](../data/examples.txt)**\n",
    "* Example documentation from **[docs_sample](../docs_sample/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_prompt(chunk, n_questions=3):\n",
    "    questions = '\\n'.join(random.sample(real_queries, n_questions))\n",
    "    user_prompt = prompt_template.format(QUESTIONS=questions, CHUNK=chunk)\n",
    "    return user_prompt\n",
    "\n",
    "user_prompt = generate_context_prompt(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "I want to group on a certain column. Why cannot I find the column in the drop-down menu？It seems that not all     \n",
       "colunms can be found in the drop-down menu. Hi, I m training for 100 epochs and logging results and I see around   \n",
       "200 steps on wandb dashboard. is epoch not same as step ? How do I get an artifact's metadata without downloading  \n",
       "the artifact ?                                                                                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "In the next step, you will read a fragment of W&amp;B documentation. This will serve as inspiration for synthetic user \n",
       "question and the source of the answer. Here is the document fragment:                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   \n",
       "         data = [[wandb.Image(x_i), y_i, y_pred] f                                                                 </span>\n",
       "<span style=\"background-color: #272822\">             or x_i, y_i, y_pred in list(zip(x[:n], y[:n], outputs[:n]))]                                          </span>\n",
       "<span style=\"background-color: #272822\">         wandb_logger.log_table(                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">             key='sample_table',                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">             columns=columns,                                                                                      </span>\n",
       "<span style=\"background-color: #272822\">             data=data)                                                                                            </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   \n",
       "</span>\n",
       "...                                                                                                                \n",
       "\n",
       "trainer = pl.Trainer( ... callbacks=[LogPredictionSamplesCallback()] )                                             \n",
       "\n",
       "<span style=\"background-color: #272822\">                                                                                                                   \n",
       "                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> ### How to use multiple GPUs with Lightning and W&amp;B?                                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> PyTorch Lightning has Multi-GPU support through their DDP Interface. However, PyTorch Lightning's design requires </span>\n",
       "<span style=\"background-color: #272822\"> us to be careful about how we instantiate our GPUs.                                                               </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> Lightning assumes that each GPU (or Rank) in your training loop must be instantiated in exactly the same way - wi </span>\n",
       "<span style=\"background-color: #272822\"> the same initial conditions. However, only rank 0 process gets access to the `wandb.run` object, and for non-zero </span>\n",
       "<span style=\"background-color: #272822\"> rank processes: `wandb.run = None`. This could cause your non-zero processes to fail. Such a situation can put yo </span>\n",
       "<span style=\"background-color: #272822\"> in a **deadlock** because rank 0 process will wait for the non-zero rank processes to join, which have already    </span>\n",
       "<span style=\"background-color: #272822\"> crashed.                                                                                                          </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> For this reason, we have to be careful about how we set up our training code. The recommended way to set it up    </span>\n",
       "<span style=\"background-color: #272822\"> would be to have your code be independent of the `wandb.run` object.                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> ```python                                                                                                         </span>\n",
       "<span style=\"background-color: #272822\"> class MNISTClassifier(pl.LightningModule):                                                                        </span>\n",
       "<span style=\"background-color: #272822\">     def __init__(self):                                                                                           </span>\n",
       "<span style=\"background-color: #272822\">         super(MNISTClassifier, self).__init__()                                                                   </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         self.model = nn.Sequential(                                                                               </span>\n",
       "<span style=\"background-color: #272822\">             nn.Flatten(),                                                                                         </span>\n",
       "<span style=\"background-color: #272822\">             nn.Linear(28 * 28, 128),                                                                              </span>\n",
       "<span style=\"background-color: #272822\">             nn.ReLU(),                                                                                            </span>\n",
       "<span style=\"background-color: #272822\">             nn.Linear(128, 10),                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         )                                                                                                         </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         self.loss = nn.CrossEntropyLoss()                                                                         </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">     def forward(self, x):                                                                                         </span>\n",
       "<span style=\"background-color: #272822\">         return self.model(x)                                                                                      </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">     def training_step(self, batch, batch_idx):                                                                    </span>\n",
       "<span style=\"background-color: #272822\">         x, y = batch                                                                                              </span>\n",
       "<span style=\"background-color: #272822\">         y_hat = self.forward(x)                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         loss = self.loss(y_hat, y)                                                                                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         self.log(\"train/loss\", loss)                                                                              </span>\n",
       "<span style=\"background-color: #272822\">         return {\"train_loss\": loss}                                                                               </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">     def validation_step(self, batch, batch_idx):                                                                  </span>\n",
       "<span style=\"background-color: #272822\">         x, y = batch                                                                                              </span>\n",
       "<span style=\"background-color: #272822\">         y_hat = self.forward(x)                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         loss = self.loss(y_hat, y)                                                                                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">         self.log(\"val/loss\", loss)                                                                                </span>\n",
       "<span style=\"background-color: #272822\">         return {\"val_loss\": loss}                                                                                 </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">     def configure_optimizers(self):                                                                               </span>\n",
       "<span style=\"background-color: #272822\">         return torch.optim.Adam(self.parameters(), lr=0.001)                                                      </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> def main():                                                                                                       </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> ***                                                                                                               </span>\n",
       "<span style=\"background-color: #272822\"> You will now generate a user question and corresponding answer based on the above document.                       </span>\n",
       "<span style=\"background-color: #272822\"> First, explain the user context and what problems they might be trying to solve.                                  </span>\n",
       "<span style=\"background-color: #272822\"> Second, generate user question.                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> Third, provide the accurate and concise answer in markdown format to the user question using the documentation.   </span>\n",
       "<span style=\"background-color: #272822\"> You'll be evaluated on:                                                                                           </span>\n",
       "<span style=\"background-color: #272822\"> - how realistic is that this question will come from a real user one day?                                         </span>\n",
       "<span style=\"background-color: #272822\"> - is this question about W&amp;B?                                                                                     </span>\n",
       "<span style=\"background-color: #272822\"> - can the question be answered using the W&amp;B document fragment above?                                             </span>\n",
       "<span style=\"background-color: #272822\"> - how accurate is the answer?                                                                                     </span>\n",
       "<span style=\"background-color: #272822\"> Remember that users have different styles and can be imprecise. You are very good at impersonating them!          </span>\n",
       "<span style=\"background-color: #272822\"> Use the following format:                                                                                         </span>\n",
       "<span style=\"background-color: #272822\"> CONTEXT:                                                                                                          </span>\n",
       "<span style=\"background-color: #272822\"> QUESTION:                                                                                                         </span>\n",
       "<span style=\"background-color: #272822\"> ANSWER:                                                                                                           </span>\n",
       "<span style=\"background-color: #272822\"> Let's start!                                                                                                      </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   \n",
       "</span></pre>\n"
      ],
      "text/plain": [
       "Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "I want to group on a certain column. Why cannot I find the column in the drop-down menu？It seems that not all     \n",
       "colunms can be found in the drop-down menu. Hi, I m training for 100 epochs and logging results and I see around   \n",
       "200 steps on wandb dashboard. is epoch not same as step ? How do I get an artifact's metadata without downloading  \n",
       "the artifact ?                                                                                                     \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "In the next step, you will read a fragment of W&B documentation. This will serve as inspiration for synthetic user \n",
       "question and the source of the answer. Here is the document fragment:                                              \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \n",
       "\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        data = [[wandb.Image(x_i), y_i, y_pred] f                                                                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m            or x_i, y_i, y_pred in list(zip(x[:n], y[:n], outputs[:n]))]                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        wandb_logger.log_table(                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m            key='sample_table',                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m            columns=columns,                                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m            data=data)                                                                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \n",
       "\u001b[0m\n",
       "...                                                                                                                \n",
       "\n",
       "trainer = pl.Trainer( ... callbacks=[LogPredictionSamplesCallback()] )                                             \n",
       "\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \n",
       "\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m### How to use multiple GPUs with Lightning and W&B?                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mPyTorch Lightning has Multi-GPU support through their DDP Interface. However, PyTorch Lightning's design requires\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mus to be careful about how we instantiate our GPUs.                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mLightning assumes that each GPU (or Rank) in your training loop must be instantiated in exactly the same way - wi\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mthe same initial conditions. However, only rank 0 process gets access to the `wandb.run` object, and for non-zero\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mrank processes: `wandb.run = None`. This could cause your non-zero processes to fail. Such a situation can put yo\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34min a **deadlock** because rank 0 process will wait for the non-zero rank processes to join, which have already   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mcrashed.                                                                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mFor this reason, we have to be careful about how we set up our training code. The recommended way to set it up   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mwould be to have your code be independent of the `wandb.run` object.                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m```python                                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mclass MNISTClassifier(pl.LightningModule):                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    def __init__(self):                                                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        super(MNISTClassifier, self).__init__()                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        self.model = nn.Sequential(                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m            nn.Flatten(),                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m            nn.Linear(28 * 28, 128),                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m            nn.ReLU(),                                                                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m            nn.Linear(128, 10),                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        )                                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        self.loss = nn.CrossEntropyLoss()                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    def forward(self, x):                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        return self.model(x)                                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    def training_step(self, batch, batch_idx):                                                                   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        x, y = batch                                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        y_hat = self.forward(x)                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        loss = self.loss(y_hat, y)                                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        self.log(\"train/loss\", loss)                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        return {\"train_loss\": loss}                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    def validation_step(self, batch, batch_idx):                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        x, y = batch                                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        y_hat = self.forward(x)                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        loss = self.loss(y_hat, y)                                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        self.log(\"val/loss\", loss)                                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        return {\"val_loss\": loss}                                                                                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    def configure_optimizers(self):                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m        return torch.optim.Adam(self.parameters(), lr=0.001)                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mdef main():                                                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m***                                                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mYou will now generate a user question and corresponding answer based on the above document.                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mFirst, explain the user context and what problems they might be trying to solve.                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mSecond, generate user question.                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mThird, provide the accurate and concise answer in markdown format to the user question using the documentation.  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mYou'll be evaluated on:                                                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m- how realistic is that this question will come from a real user one day?                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m- is this question about W&B?                                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m- can the question be answered using the W&B document fragment above?                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m- how accurate is the answer?                                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mRemember that users have different styles and can be imprecise. You are very good at impersonating them!         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mUse the following format:                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mCONTEXT:                                                                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mQUESTION:                                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mANSWER:                                                                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34mLet's start!                                                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \n",
       "\u001b[0m"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we request the model to generate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(documents, n_questions=3, n_generations=5):\n",
    "    questions = []\n",
    "    for _, document in documents:\n",
    "        # Extract random chunck from a W&B document\n",
    "        chunk = extract_random_chunk(document)\n",
    "        # Fill in prompt_template with example questions and docs chunck\n",
    "        user_prompt = generate_context_prompt(chunk, n_questions)\n",
    "        # Pass system_prompt and user_prompt to LLM\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        response = completion_with_backoff(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            n = n_generations,\n",
    "            )\n",
    "        # Produce n responses from input prompt\n",
    "        questions.extend([response.choices[i].message.content for i in range(n_generations)])\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse model generation and extract CONTEXT, QUESTION and ANSWER\n",
    "def parse_generation(generation):\n",
    "    lines = generation.split(\"\\n\")\n",
    "    context = []\n",
    "    question = []\n",
    "    answer = []\n",
    "    flag = None\n",
    "    \n",
    "    for line in lines:\n",
    "        if \"CONTEXT:\" in line:\n",
    "            flag = \"context\"\n",
    "            line = line.replace(\"CONTEXT:\", \"\").strip()\n",
    "        elif \"QUESTION:\" in line:\n",
    "            flag = \"question\"\n",
    "            line = line.replace(\"QUESTION:\", \"\").strip()\n",
    "        elif \"ANSWER:\" in line:\n",
    "            flag = \"answer\"\n",
    "            line = line.replace(\"ANSWER:\", \"\").strip()\n",
    "\n",
    "        if flag == \"context\":\n",
    "            context.append(line)\n",
    "        elif flag == \"question\":\n",
    "            question.append(line)\n",
    "        elif flag == \"answer\":\n",
    "            answer.append(line)\n",
    "\n",
    "    context = \"\\n\".join(context)\n",
    "    question = \"\\n\".join(question)\n",
    "    answer = \"\\n\".join(answer)\n",
    "    return context, question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions using LLM\n",
    "generations = generate_questions([documents[0]], n_questions=3, n_generations=5)\n",
    "parse_generation(generations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Above generated text is split into `Context`, `Question`, `Answer`\n",
    "* Question looks better that with previous approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we verified that function works, we can run it in a loop to generate questions\n",
    "\n",
    "Below.. cause we want a big dataset of synthetic questions for our model evaluation:\n",
    "* we save LLM generations into a dataframe and a csv, \n",
    "* we log this as a W&B Table and save the csv as a W&B Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_generations = []\n",
    "generations = generate_questions(documents, n_questions=3, n_generations=5)\n",
    "for generation in generations:\n",
    "    context, question, answer = parse_generation(generation)\n",
    "    parsed_generations.append({\"context\": context, \"question\": question, \"answer\": answer})\n",
    "\n",
    "# let's convert parsed_generations to a pandas dataframe and save it locally\n",
    "df = pd.DataFrame(parsed_generations)\n",
    "df.to_csv('generated_examples.csv', index=False)\n",
    "\n",
    "# log df as a table to W&B for interactive exploration\n",
    "wandb.log({\"generated_examples\": wandb.Table(dataframe=df)})\n",
    "\n",
    "# log csv file as an artifact to W&B for later use\n",
    "artifact = wandb.Artifact(\"generated_examples\", type=\"dataset\")\n",
    "artifact.add_file(\"generated_examples.csv\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
