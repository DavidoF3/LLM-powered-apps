{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "from pprint import pprint\n",
    "from getpass import getpass\n",
    "from wandb.integration.openai import autolog\n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set OpenAI API key \n",
    "\n",
    "To get key, click on [link](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter password in the VS Code prompt at the top of your VS Code window!\n",
      "LLAMA2 API key configured\n"
     ]
    }
   ],
   "source": [
    "# Set LLAMA2 API key environment variable\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "  os.environ[\"LLAMA2_API_KEY\"] = getpass(\"Paste your LLAM2 key from your huggingface settings \\n\")\n",
    "  # openai.api_key = os.getenv(\"LLAMA2_API_KEY\", \"\")\n",
    "\n",
    "assert os.getenv(\"LLAMA2_API_KEY\", \"\").startswith(\"hf_\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "print(\"LLAMA2 API key configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# model_id = 'meta-llama/Llama-2-70b-chat-hf'\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_id, token=os.getenv(\"LLAMA2_API_KEY\", \"\"))\n",
    "# model = LlamaForCausalLM.from_pretrained(model_id, token=os.getenv(\"LLAMA2_API_KEY\", \"\"))\n",
    "\n",
    "\n",
    "# # import transformers \n",
    "\n",
    "# # model_id = 'meta-llama/Llama-2-70b-chat-hf'\n",
    "\n",
    "# # # begin initializing HF items, need auth token for these\n",
    "# # hf_auth = os.getenv(\"LLAMA2_API_KEY\", \"\")\n",
    "# # model_config = transformers.AutoConfig.from_pretrained(\n",
    "# #     model_id,\n",
    "# #     use_auth_token=hf_auth\n",
    "# # )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start W&B logging\n",
    "\n",
    "autolog - convenient function for logging OpenAI results to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33md-oliver-cort\u001b[0m (\u001b[33mdoc93\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/davidoc/Documents/personal/LLM-powered-apps/notebooks/wandb/run-20230818_004135-h960csz8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/doc93/llmapps/runs/h960csz8' target=\"_blank\">young-shape-1</a></strong> to <a href='https://wandb.ai/doc93/llmapps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/doc93/llmapps' target=\"_blank\">https://wandb.ai/doc93/llmapps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/doc93/llmapps/runs/h960csz8' target=\"_blank\">https://wandb.ai/doc93/llmapps/runs/h960csz8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "autolog({\"project\":\"llmapps\", \"job_type\": \"introduction\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "The Llama 2 7B models were trained using the Llama 2 7B tokenizer - used to tokenise text and decode tokenised text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define llama2 model to load\n",
    "model_id = 'meta-llama/Llama-2-70b-chat-hf'\n",
    "\n",
    "# Get the HF auth token\n",
    "hf_auth = os.getenv(\"LLAMA2_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidoc/Documents/personal/LLM-powered-apps/venv_llm/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:631: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1334, 5861, 669, 3457, 2129, 338, 29663, 29991]\n",
      "<s> Weights & Biases is awesome!\n"
     ]
    }
   ],
   "source": [
    "# Example of tokanisation (encode and decode)\n",
    "# - here we use the tokeniser for the llama2 model\n",
    "# - different models may need a different tokeniser\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    "    )\n",
    "enc = tokenizer.encode(\"Weights & Biases is awesome!\")\n",
    "print(enc)\n",
    "print(tokenizer.decode(enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t<s>\n",
      "1334\tWe\n",
      "5861\tights\n",
      "669\t&\n",
      "3457\tBi\n",
      "2129\tases\n",
      "338\tis\n",
      "29663\tawesome\n",
      "29991\t!\n"
     ]
    }
   ],
   "source": [
    "# Decode tokens one by one\n",
    "# - most natural way of splitting words into tokens would be to use the spaces\n",
    "# - however, to contain the size of the vocabulary, some words split into sub-words (units)\n",
    "for token_id in enc:\n",
    "    print(f\"{token_id}\\t{tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "### Sampling with Temperature\n",
    "\n",
    "Let's sample some text from the model by passing the **temperature** parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a function that passes temperature parameter to the OpenAI completion function (API)\n",
    "# def generate_with_temperature(temp):\n",
    "#   \"Generate text with a given temperature, higher temperature means more randomness\"\n",
    "#   response = openai.Completion.create(\n",
    "#     model=\"text-davinci-003\",\n",
    "#     prompt=\"Say something about Weights & Biases\",\n",
    "#     max_tokens=50,\n",
    "#     temperature=temp,\n",
    "#   )\n",
    "#   return response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Set quantization configuration to load large model with less GPU memory\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# - this requires the `bitsandbytes` library\u001b[39;00m\n\u001b[1;32m      3\u001b[0m bnb_config \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mBitsAndBytesConfig(\n\u001b[1;32m      4\u001b[0m     load_in_4bit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     bnb_4bit_quant_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnf4\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     bnb_4bit_use_double_quant\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m----> 7\u001b[0m     bnb_4bit_compute_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbfloat16\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Set quantization configuration to load large model with less GPU memory\n",
    "# - this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that passes temperature parameter to the llama2 completion function (API)\n",
    "def generate_with_temperature(temp):\n",
    "  \"Generate text with a given temperature, higher temperature means more randomness\"\n",
    "  response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Say something about Weights & Biases\",\n",
    "    max_tokens=50,\n",
    "    temperature=temp,\n",
    "  )\n",
    "  return response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with different temperature values\n",
    "# - with temperatures > 1, the generated text becomes too gibberish (cause more low probability tokens get sampled)\n",
    "for temp in [0, 0.5, 1, 1.5, 2]:\n",
    "  pprint(f'TEMP: {temp}, GENERATION: {generate_with_temperature(temp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top p sampling\n",
    "\n",
    "Can also use the [`top_p` parameter](https://platform.openai.com/docs/api-reference/completions/create#completions/create-top_p) to control the diversity of the generated text. \n",
    "- This parameter controls the cumulative probability of the next token. \n",
    "- For example, if `top_p=0.9`, the model will pick the next token from the top 90% most likely tokens. \n",
    "- The higher the `top_p` the more likely the model will pick a token that it hasn't seen before. \n",
    "- Decreasing `top_p` results in higher probability text being generated.\n",
    "- You should only use one of `temperature` or `top_p` at a given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_topp(topp):\n",
    "  \"Generate text with a given top-p, higher top-p means more randomness\"\n",
    "  response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Say something about Weights & Biases\",\n",
    "    max_tokens=50,\n",
    "    top_p=topp,\n",
    "    )\n",
    "  return response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topp in [0.01, 0.1, 0.5, 1]:\n",
    "  pprint(f'TOP_P: {topp}, GENERATION: {generate_with_topp(topp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat API\n",
    "\n",
    "- The Chat API (using `gpt-3.5-turbo`) looks a bit different to the one above (used with the `davinci-003` model).\n",
    "  \n",
    "- The `gpt-3.5-turbo` model is faster and cheaper than `davinci-003`\n",
    "  \n",
    "- Instead of a prompt, this API takes a list of messages.\n",
    "  - Messages come with different roles (system or user) and corresponding prompt\n",
    "  - `system-role` gives some control over the model's response (below we steer the model to adhere to a certain behaviour)\n",
    "\n",
    "\n",
    "- See [OpenAI ref](https://platform.openai.com/docs/guides/gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Say something about Weights & Biases\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# The response is a JSON object with relevant information about the request.\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish wandb run (see [Promps docs](https://docs.wandb.ai/guides/prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
