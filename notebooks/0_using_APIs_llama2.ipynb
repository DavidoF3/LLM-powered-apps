{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "from pprint import pprint\n",
    "from getpass import getpass\n",
    "from rich.markdown import Markdown\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set llama2 API key \n",
    "\n",
    "To get key, click on [link](https://huggingface.co/settings/tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter password in the VS Code prompt at the top of your VS Code window!\n",
      "Llama2 API key configured\n"
     ]
    }
   ],
   "source": [
    "# Set LLAMA2 API key environment variable\n",
    "if os.getenv(\"LLAMA2_API_KEY\") is None:\n",
    "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "  os.environ[\"LLAMA2_API_KEY\"] = getpass(\"Paste your LLAM2 key from your huggingface settings \\n\")\n",
    "\n",
    "assert os.getenv(\"LLAMA2_API_KEY\", \"\").startswith(\"hf_\"), \"This doesn't look like a valid HuggingFace llama2 key\"\n",
    "print(\"Llama2 API key configured\")\n",
    "\n",
    "# Get the HF auth token\n",
    "hf_auth = os.getenv(\"LLAMA2_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "The Llama 2 7B models were trained using the Llama 2 7B tokenizer - used to tokenise text and decode tokenised text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define llama2 model to load\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1334, 5861, 669, 3457, 2129, 338, 29663, 29991]\n",
      "<s> Weights & Biases is awesome!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dolivercortadellas/Training/LLM-powered-apps/venv_llm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example of tokanisation (encode and decode)\n",
    "# - here we download the tokeniser for the llama2 model\n",
    "# - different models may need a different tokeniser\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    "    )\n",
    "enc = tokenizer.encode(\"Weights & Biases is awesome!\")\n",
    "print(enc)\n",
    "print(tokenizer.decode(enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t<s>\n",
      "1334\tWe\n",
      "5861\tights\n",
      "669\t&\n",
      "3457\tBi\n",
      "2129\tases\n",
      "338\tis\n",
      "29663\tawesome\n",
      "29991\t!\n"
     ]
    }
   ],
   "source": [
    "# Decode tokens one by one\n",
    "# - most natural way of splitting words into tokens would be to use the spaces\n",
    "# - however, to contain the size of the vocabulary, some words split into sub-words (units)\n",
    "for token_id in enc:\n",
    "    print(f\"{token_id}\\t{tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about \n",
    "* [transformers.pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines)\n",
    "* [config inputs](https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
    "* [llama2 usage - ref1](https://huggingface.co/blog/llama2)\n",
    "* [llama2 usage - ref2](https://www.pinecone.io/learn/llama-2/#Building-a-Llama-2-Conversational-Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dolivercortadellas/Training/LLM-powered-apps/venv_llm/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44283369c8549898560293ede040f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set quantization configuration to load large model with less GPU memory\n",
    "# - this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# This configuration object uses the model configuration from Hugging Face \n",
    "# to set different model parameters\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "# Download and initialize the model \n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "### Sampling with Temperature\n",
    "\n",
    "Let's sample some text from the model by passing the **temperature** parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Option 1 - a) instanciate transformers.pipeline and b) run inference by passing config inputs and prompt to object***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_generate_text = transformers.pipeline(\n",
    "    task='text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that passes temperature parameter to the llama2 api\n",
    "def generate_with_temperature(pipeline_generate_text, temp):\n",
    "    \"Generate text with a given temperature -> higher temperature means more randomness (0 - 1)\"\n",
    "    response = pipeline_generate_text(\n",
    "        'Describe the company Weights & Biases.\\n',\n",
    "        do_sample=True,           # Whether or not to use sampling \n",
    "        top_k=50,\n",
    "        num_return_sequences=1,   # number of independently computed returned sequences for each element in the batch.\n",
    "        temperature=temp,         # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "        max_new_tokens=50,\n",
    "        # repetition_penalty=1.1, # without this output begins repeating\n",
    "        )\n",
    "    return response[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dolivercortadellas/Training/LLM-powered-apps/venv_llm/lib/python3.8/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TEMP: 0.01, GENERATION: Describe the company Weights & Biases.\\n'\n",
      " '\\n'\n",
      " 'Weights & Biases is a startup that aims to help developers and data '\n",
      " 'scientists make better decisions by providing them with a platform to '\n",
      " 'collect, organize, and share their data and models. The company was founded '\n",
      " 'in 20')\n",
      "------\n",
      "('TEMP: 0.5, GENERATION: Describe the company Weights & Biases.\\n'\n",
      " '\\n'\n",
      " 'Weights & Biases is a software company that develops tools for machine '\n",
      " 'learning engineers to measure and manage the risks associated with their '\n",
      " \"models. The company's platform provides a suite of tools for model risk \"\n",
      " 'management, including model performance metrics')\n",
      "------\n",
      "('TEMP: 1.0, GENERATION: Describe the company Weights & Biases.\\n'\n",
      " '\\n'\n",
      " 'Weighted means Biases, formerly known as Weights & Biases, is a software '\n",
      " 'organization that creates equipment and administrations to help engineers '\n",
      " 'and researcher tune and test AI models. The organization was established in '\n",
      " '201')\n",
      "------\n",
      "('TEMP: 1.5, GENERATION: Describe the company Weights & Biases.\\n'\n",
      " 'Weights & Biases logo\\n'\n",
      " 'Founded in December 2017 by Diego Oppenheimer and Trevor Knauck, Weights & '\n",
      " 'Biases is a platform that helps to train artificial intelligence models '\n",
      " 'efficiently through model parallelism. The')\n",
      "------\n",
      "('TEMP: 2.0, GENERATION: Describe the company Weights & Biases.\\n'\n",
      " 'When I see that ad, I immediately get transported in the imagination space: '\n",
      " 'how exactly do Weights & Biases work?\\n'\n",
      " 'Focused and goal-driven people utilizing information.\\n'\n",
      " 'They provide insight and knowledge so users and others')\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Generate text with different temperature values\n",
    "# - as temperature increased > 1, the generated text becomes too gibberish (cause more low probability tokens get sampled)\n",
    "for temp in [0.01, 0.5, 1.0, 1.5, 2.0]:\n",
    "    pprint(f'TEMP: {temp}, GENERATION: {generate_with_temperature(pipeline_generate_text, temp)}')\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Option 2 - a) instanciate transformers.pipeline and pass config inputs and b) run inference by passing prompt to object***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Create a function that passes temperature parameter to the llama2 api\n",
    "def generate_with_temperature(model, tokenizer, temp):\n",
    "    \"Generate text with a given temperature -> higher temperature means more randomness (0 - 1)\"\n",
    "    # print(temp)\n",
    "    generate_text = transformers.pipeline(\n",
    "        task='text-generation',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        do_sample=True,           # Whether or not to use sampling \n",
    "        repetition_penalty=1.1, # without this output begins repeating\n",
    "        # return_full_text=True,  # langchain expects the full text\n",
    "        # we pass model parameters here too\n",
    "        temperature=temp,         # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "        max_new_tokens=50,        # max number of tokens to generate in the output (eg. 512)\n",
    "    )\n",
    "    response = generate_text(\"Describe the company Weights & Biases.\")\n",
    "    # response = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
    "    return response[0][\"generated_text\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Option 3 - a) tokenise prompt, b) pass that along with config inputs to model.generate, c) decode answer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Create a function that passes temperature parameter to the llama2 api\n",
    "def generate_with_temperature(model, tokenizer, temp):\n",
    "    \"Generate text with a given temperature -> higher temperature means more randomness (0 - 1)\"\n",
    "\n",
    "    prompt = \"Describe the company Weights & Biases.\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    # Generate\n",
    "    print('------')\n",
    "    generate_ids = model.generate(\n",
    "        inputs.input_ids, \n",
    "        do_sample=True,           # Whether or not to use sampling \n",
    "        repetition_penalty=1.1,\n",
    "        temperature=temp,\n",
    "        max_new_tokens=50,\n",
    "        )\n",
    "    return tokenizer.decode(generate_ids[0], skip_special_tokens=True)\n",
    "    # return tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate text with different temperature values\n",
    "# # - as temperature increased > 1, the generated text becomes too gibberish (cause more low probability tokens get sampled)\n",
    "# for temp in [0.01, 0.5, 1.0, 1.5, 2.0]:\n",
    "#   pprint(f'TEMP: {temp}, GENERATION: {generate_with_temperature(model, tokenizer, temp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top p sampling\n",
    "\n",
    "Can also use the [`top_p` parameter](https://platform.openai.com/docs/api-reference/completions/create#completions/create-top_p) to control the diversity of the generated text. \n",
    "- This parameter controls the cumulative probability of the next token. \n",
    "- For example, if `top_p=0.9`, the model will pick the next token from the top 90% most likely tokens. \n",
    "- The higher the `top_p` the more likely the model will pick a token that it hasn't seen before. \n",
    "- Decreasing `top_p` results in higher probability text being generated.\n",
    "- You should only use one of `temperature` or `top_p` at a given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that passes top_p parameter to the llama2 api\n",
    "def generate_with_topp(pipeline_generate_text, topp):\n",
    "    \"Generate text with a given temperature -> higher temperature means more randomness (0 - 1)\"\n",
    "    response = pipeline_generate_text(\n",
    "        'Describe the company Weights & Biases.\\n',\n",
    "        do_sample=True,           # Whether or not to use sampling \n",
    "        top_k=50,\n",
    "        num_return_sequences=1,   # number of independently computed returned sequences for each element in the batch.\n",
    "        top_p=topp,         # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "        max_new_tokens=50,\n",
    "        # repetition_penalty=1.1, # without this output begins repeating\n",
    "        )\n",
    "    return response[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TOP_P: 0.01, GENERATION: Describe the company Weights & Biases.\\n'\n",
      " '\\n'\n",
      " 'Weights & Biases is a startup that aims to help developers and data '\n",
      " 'scientists make better decisions by providing them with a platform to '\n",
      " 'collect, organize, and share their data and models. The company was founded '\n",
      " 'in 20')\n",
      "------\n",
      "('TOP_P: 0.1, GENERATION: Describe the company Weights & Biases.\\n'\n",
      " '\\n'\n",
      " 'Weights & Biases is a startup that aims to help developers and data '\n",
      " 'scientists make better decisions by providing them with a platform to '\n",
      " 'collect, organize, and share their data and models. The company was founded '\n",
      " 'in 20')\n",
      "------\n",
      "('TOP_P: 0.5, GENERATION: Describe the company Weights & Biases.\\n'\n",
      " '\\n'\n",
      " 'Weights & Biases is a software company that specializes in developing '\n",
      " 'machine learning tools and platforms. The company was founded in 2017 by a '\n",
      " 'team of experienced machine learning practitioners and researchers who '\n",
      " 'recognized the need for better')\n",
      "------\n",
      "('TOP_P: 1.0, GENERATION: Describe the company Weights & Biases.\\n'\n",
      " \"Weights & Biases is the world's first AI-powered creative platform for \"\n",
      " 'designers. It was built by a team of AI researchers and designers to help '\n",
      " 'designers work more effectively, efficiently, and creatively')\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for topp in [0.01, 0.1, 0.5, 1.0]:\n",
    "  pprint(f'TOP_P: {topp}, GENERATION: {generate_with_topp(pipeline_generate_text, topp)}')\n",
    "  print('------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to prompt Llama2\n",
    "\n",
    "For llama2, the prompt template for the first turn looks like the example below.\n",
    "- Messages come with different roles - system prompt and user message\n",
    "- We can use any `system_prompt` we want,\n",
    "  - But it's crucial that the format matches the one used during training (see below). \n",
    "  - The instructions between the special <<SYS>> tokens provide context for the model so it knows how we expect it to respond (adhere to a certain behaviour).\n",
    "  - The `user_message` is actually sent to the language model when the user enters some text\n",
    "\n",
    "------------------------------\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "{{ system_prompt }}\n",
    "<</SYS>>\n",
    "\n",
    "{{ user_message }} [/INST]\n",
    "```\n",
    "\n",
    "------------------------------\n",
    "\n",
    "See [Lama2 ref](https://huggingface.co/blog/llama2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "# \"\"\"\n",
    "# user_message = \"\"\"There's a llama in my garden. What should I do?\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful assistant.\"\"\"\n",
    "user_message = \"\"\"Describe the company Weights & Biases.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>[INST] <<SYS>>\\n'\n",
      " 'You are a helpful assistant.\\n'\n",
      " '<</SYS>>\\n'\n",
      " '\\n'\n",
      " 'Describe the company Weights & Biases.[/INST]')\n"
     ]
    }
   ],
   "source": [
    "# special tokens used by llama 2 chat\n",
    "B_INST = \"[INST] \"\n",
    "E_INST = \"[/INST]\"\n",
    "B_SYS  = \"<<SYS>>\\n\"\n",
    "E_SYS  = \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "# create the system message\n",
    "sys_msg = f\"<s>{B_INST}{B_SYS}{system_prompt}{E_SYS}\"\n",
    "# Create user message\n",
    "user_msg = f\"{user_message}{E_INST}\"\n",
    "\n",
    "prompt = sys_msg + user_msg\n",
    "pprint(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that passes top_p parameter to the llama2 api\n",
    "def generate_with_prompt(pipeline_generate_text, prompt):\n",
    "    \"Generate text with a given temperature -> higher temperature means more randomness (0 - 1)\"\n",
    "    response = pipeline_generate_text(\n",
    "        prompt,\n",
    "        do_sample=True,           # Whether or not to use sampling \n",
    "        top_k=50,\n",
    "        num_return_sequences=1,   # number of independently computed returned sequences for each element in the batch.\n",
    "        temperature=0.01,         # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "        max_new_tokens=200,\n",
    "        repetition_penalty=1.1, # without this output begins repeating\n",
    "        )\n",
    "    return response[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>[INST] <<SYS>>\\n'\n",
      " 'You are a helpful assistant.\\n'\n",
      " '<</SYS>>\\n'\n",
      " '\\n'\n",
      " 'Describe the company Weights & Biases.[/INST]  Of course! Weights & Biases '\n",
      " 'is a cutting-edge technology company that specializes in developing '\n",
      " 'innovative machine learning tools and platforms. The company was founded in '\n",
      " '2017 by a team of experienced machine learning researchers and engineers who '\n",
      " 'were passionate about creating practical solutions for the industry.\\n'\n",
      " \"Weights & Biases' mission is to empower data scientists and machine learning \"\n",
      " 'practitioners with the most advanced tools and techniques to build better '\n",
      " 'models, faster. They believe that by providing a platform that simplifies '\n",
      " 'the development, deployment, and management of machine learning models, they '\n",
      " 'can help organizations of all sizes unlock their full potential.\\n'\n",
      " 'The company\\'s flagship product is an AI-powered platform called \"W&B,\" '\n",
      " 'which provides a wide range of features such as:\\n'\n",
      " '* Automated model optimization: W&B uses AI algorithms to automatically '\n",
      " 'optimize machine learning models for performance, reducing the time and '\n",
      " 'effort required to develop and deploy models.')\n"
     ]
    }
   ],
   "source": [
    "answer = generate_with_prompt(pipeline_generate_text, prompt)\n",
    "pprint(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish wandb run (see [Promps docs](https://docs.wandb.ai/guides/prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W&B Trace with LangChain Agent\n",
    "\n",
    "Check [W&B Prompt tracing](https://docs.wandb.ai/guides/prompts/quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33md-oliver-cort\u001b[0m (\u001b[33mdoc93\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dolivercortadellas/Training/LLM-powered-apps/notebooks/wandb/run-20230826_214843-m3q6iin6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/doc93/llmapps/runs/m3q6iin6' target=\"_blank\">tough-night-14</a></strong> to <a href='https://wandb.ai/doc93/llmapps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/doc93/llmapps' target=\"_blank\">https://wandb.ai/doc93/llmapps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/doc93/llmapps/runs/m3q6iin6' target=\"_blank\">https://wandb.ai/doc93/llmapps/runs/m3q6iin6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Option 1 -----------------------------------------------------------------------------------------\n",
    "# Turn on automated W&B logging with LangChain\n",
    "# - Now any call to a LangChain LLM, Chain, Tool or Agent will be logged to Weights & Biases.\n",
    "\"\"\"\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "\"\"\"\n",
    "\n",
    "# Option 2 -----------------------------------------------------------------------------------------\n",
    "# Alternatively, you might prefer to use a context manager to decide which LLM runs to log to W&B\n",
    "# - in which case, run the following\n",
    "from langchain.callbacks import wandb_tracing_enabled\n",
    "\n",
    "if \"LANGCHAIN_WANDB_TRACING\" in os.environ:\n",
    "    del os.environ[\"LANGCHAIN_WANDB_TRACING\"]\n",
    "# Then, to log an LLM run, we should write the following code (math_agent is in the context of the code below)\n",
    "\"\"\"\n",
    "with wandb_tracing_enabled():\n",
    "    math_agent.run(\"What is 5 raised to .123243 power?\")\n",
    "\"\"\"\n",
    "\n",
    "# Set parameters that are typically passed to wandb.init()\n",
    "run = wandb.init(project=\"llmapps\", job_type=\"generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    task='text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,    # langchain expects the full text\n",
    "    # we pass model parameters here too\n",
    "    do_sample=True,           # Whether or not to use sampling \n",
    "    temperature=0.5,          # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=50,        # max number of tokens to generate in the output (eg. 512)\n",
    "    repetition_penalty=1.1,   # without this output begins repeating\n",
    ")\n",
    "# response = generate_text(\"Describe the company Weights & Biases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the initialised Hugging Face pipeline into LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `llm` we can now use Llama 2 with `LangChain`'s advanced tooling such as `agents`, `chains`, and `callbacks`. \n",
    "\n",
    "## Create a standard math Agent using LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "math_agent = initialize_agent(tools, \n",
    "                              llm, \n",
    "                              agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The square root of 5.4 is 2.169.\n",
      "----\n",
      "Could not parse LLM output: ` Hmm, looks like we can't use the calculator for this problem. Let's try using the formula instead.\n",
      "Action: Write down the formula for raising a number to the power of pi, which is \"a^π = a`\n",
      "eeeee\n",
      "-0.695\n",
      "\n",
      "Please provide more details or clarify your question.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# some sample maths questions\n",
    "questions = [\n",
    "  \"Find the square root of 5.4.\",\n",
    "  \"What is 3 divided by 7.34 raised to the power of pi?\",\n",
    "  \"What is the sin of 0.47 radians, divided by the cube root of 27?\"\n",
    "]\n",
    "\n",
    "# Log LLM results into W&B\n",
    "with wandb_tracing_enabled():\n",
    "  \n",
    "  for question in questions:\n",
    "    try:\n",
    "      # call your Agent as normal\n",
    "      answer = math_agent.run(question)\n",
    "      print(answer)\n",
    "      print('----')\n",
    "    except Exception as e:\n",
    "      # any errors will be also logged to Weights & Biases\n",
    "      print(e)\n",
    "      print('eeeee')\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9e1cb989824cd5b2d9f23562ad65f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-terrain-13</strong> at: <a href='https://wandb.ai/doc93/llmapps/runs/ohj5oxem' target=\"_blank\">https://wandb.ai/doc93/llmapps/runs/ohj5oxem</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230826_214124-ohj5oxem/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfinished code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a conversational agent using LangChain\n",
    "\n",
    "We need a few things, like conversational `memory`, access to `tools`, and an `llm` (already initialized above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
    ")\n",
    "tools = load_tools([\"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize our conversational agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# initialize agent\n",
    "agent = initialize_agent(\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    early_stopping_method=\"generate\",\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `system message`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens used by llama 2 chat\n",
    "B_INST = \"[INST] \"\n",
    "E_INST = \"[/INST]\"\n",
    "B_SYS  = \"<<SYS>>\\n\"\n",
    "E_SYS  = \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "# create the system message\n",
    "sys_msg = \"<s>\" + B_SYS + \"\"\"You are a helpful assistant.\"\"\" + E_SYS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting our new system message we can move on to the prompt template for `user messages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_msg = B_INST + \" Describe the company Weights & Biases. \" + E_INST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<s><<SYS>>\\nYou are a helpful assistant.\\n<</SYS>>\\n\\n'\n",
      "'[INST]  Describe the company Weights & Biases. [/INST]'\n"
     ]
    }
   ],
   "source": [
    "pprint(sys_msg)\n",
    "pprint(user_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load system message to LangChain\n",
    "new_prompt = agent.agent.create_prompt(\n",
    "    system_message=sys_msg,\n",
    "    tools=tools\n",
    "    )\n",
    "agent.agent.llm_chain.prompt = new_prompt\n",
    "\n",
    "# Load yser message to LangChain\n",
    "agent.agent.llm_chain.prompt.messages[2].prompt.template = user_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`run` supported with either positional arguments or keyword arguments, but none were provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/Training/LLM-powered-apps/venv_llm/lib/python3.8/site-packages/langchain/chains/base.py:450\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    446\u001b[0m         _output_key\n\u001b[1;32m    447\u001b[0m     ]\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 450\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[1;32m    454\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    456\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but not both. Got args: \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m and kwargs: \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `run` supported with either positional arguments or keyword arguments, but none were provided."
     ]
    }
   ],
   "source": [
    "agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='<s><<SYS>>\\nYou are a helpful assistant.\\n<</SYS>>\\n\\n', template_format='f-string', validate_template=True), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history'),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='[INST]  Describe the company Weights & Biases. [/INST]', template_format='f-string', validate_template=True), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.agent.llm_chain.prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
