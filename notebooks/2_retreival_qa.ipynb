{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Retreival Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "from getpass import getpass\n",
    "from rich.markdown import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set OpenAI API key \n",
    "\n",
    "To get key, click on [link](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter password in the VS Code prompt at the top of your VS Code window!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "This doesn't look like a valid OpenAI API key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPlease enter password in the VS Code prompt at the top of your VS Code window!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m   os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m getpass(\u001b[39m\"\u001b[39m\u001b[39mPaste your OpenAI key from: https://platform.openai.com/account/api-keys\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[39massert\u001b[39;00m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39msk-\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mThis doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt look like a valid OpenAI API key\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOpenAI API key configured\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: This doesn't look like a valid OpenAI API key"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "print(\"OpenAI API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "\n",
    "[LangChain](https://docs.langchain.com/docs/) is a framework for developing applications powered by LLMs. We will use some of its features in the code below:\n",
    "* For processing and parsing documents.\n",
    "* Use the retreival chain - containing a lot of functionality to implement our question-answering system.\n",
    "\n",
    "Let's start by configuring W&B tracing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a single line of code to start tracing langchain with W&B\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "\n",
    "# wandb documentation to configure wandb using env variables\n",
    "# https://docs.wandb.ai/guides/track/advanced/environment-variables\n",
    "# here we are configuring the wandb project name\n",
    "os.environ[\"WANDB_PROJECT\"] = \"llmapps\"\n",
    "\n",
    "# os.environ[\"RUST_BACKTRACE\"] = \"full\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing documents\n",
    "\n",
    "We will use a small sample of markdown documents in this notebook. Let's find them and make sure we can stuff them into the prompt. That means they may need to be chunked and not exceed some number of tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with davinci model cause it uses a simpler prompt (later move on to gpt-4)\n",
    "MODEL_NAME = \"text-davinci-003\"\n",
    "# MODEL_NAME = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step of parsing our documents is to load all the Markdown files in \n",
    "# specified directory\n",
    "# - we do this by using class from LangChain -> DirectoryLoader\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "def find_md_files(directory):\n",
    "    \"Find all markdown files in a directory and return a LangChain Document\"\n",
    "    dl = DirectoryLoader(directory, \"**/*.md\")\n",
    "    return dl.load()\n",
    "\n",
    "# Load all the Markdown\n",
    "documents = find_md_files('../docs_sample/')\n",
    "\n",
    "# Number of documents\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to count tokens in the documents. For that we need a tokenizer\n",
    "tokenizer = tiktoken.encoding_for_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of tokens in each document\n",
    "def count_tokens(documents):\n",
    "    token_counts = [len(tokenizer.encode(document.page_content)) for document in documents]\n",
    "    return token_counts\n",
    "\n",
    "count_tokens(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above result, we can see that some documents are pretty short and others are quite long, and may want to chunk them into sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `LangChain` built in `MarkdownTextSplitter` to split the documents into sections (since docs are in `Markdown` format). \n",
    "* Splitting `Markdown` without breaking syntax is not that easy. This splitter strips out `syntax`.\n",
    "* The `MarkdownTextSplitter` also takes care of removing double line breaks and save us some tokens that way.\n",
    "  \n",
    "We can pass:\n",
    "* `chunk_size` param - to avoid lenghty chunks.\n",
    "* `chunk_overlap` param - useful so you don't cut sentences randomly (less necessary with Markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "md_text_splitter = MarkdownTextSplitter(chunk_size=1000)\n",
    "document_sections = md_text_splitter.split_documents(documents)\n",
    "len(document_sections), max(count_tokens(document_sections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above splitting results in 90 documents (i.e. more chunks/ documents), and the maximum number of tokens in a chunk (or document) is 537. This will fit inside context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we look at the first section\n",
    "Markdown(document_sections[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Now we use embeddings with a vector database retriever to find relevant documents for a query. \n",
    "\n",
    "We use:\n",
    "* `OpenAIEmbeddings` to embed the text, \n",
    "* `Chroma` as vector store to store the embeddings\n",
    "\n",
    "Could use different embeddings.\n",
    "* `Cohere` provides a good multilingual embedding model if dealing with languages other than English "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Initialise embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# Use Chroma vector store to parse the document chunks from above\n",
    "db = Chroma.from_documents(document_sections, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a retriever from the db. \n",
    "\n",
    "The `k` param is used to decide how many relevant sections we retrieve from the similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs=dict(k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive the docs relevant to the query by using the above Chroma retreiver \n",
    "query = \"How can I share my W&B report with my team members in a public W&B project?\"\n",
    "docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the results\n",
    "for doc in docs:\n",
    "    print(doc.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above results show that the right kind of documents are retrieved (about collaboration - related to query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff Prompt\n",
    "\n",
    "Now that we retrieved relevant docs, we want to stuff them into the prompt template along with the user query, and pass into an LLM to obtain the answer.\n",
    "\n",
    "* To do this we use the `PromptTemplate` from `LangChain` (similar to an F string in Python)\n",
    "* This is a simple prompt (not a Level 5 prompt)\n",
    "* Define two variables: `context` and `question`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "# In the prompt template we define two inputs: \"context\", \"question\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# The context is a concatenation of the retrieved docs\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "# Populate the prompt with the context and query variables\n",
    "prompt = PROMPT.format(context=context, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use langchain to call OpenAI davinci API, to predict an answer to the above prompt, given the docs retrieved from embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "response = llm.predict(prompt)\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that we stream LangChain activity into W&B (since we previously set `LANGCHAIN_WANDB_TRACING`=true). This will be useful to check what worked, any errors, and type of results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LangChain\n",
    "\n",
    "`LangChain` provides tools (like `RetrievalQA` chain) to encapsulate the above sequence of actions into a chain, in few lines of code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Instanciate this retrieval QA chain from the OpenAI LLM\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "# Run above query against this chain\n",
    "# - will retrive the most relevant docs (k=3) to the query\n",
    "# - will concatenate docs to query for improved answer?\n",
    "result = qa.run(query)\n",
    "\n",
    "# We should see a similar answer to what we saw before\n",
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
